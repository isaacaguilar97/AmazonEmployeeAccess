
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> num_cores <- parallel::detectCores() #How many cores do I have?
> cl <- makePSOCKcluster(num_cores)
> registerDoParallel(cl)
> 
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.0
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ purrr::accumulate() masks foreach::accumulate()
✖ dplyr::filter()     masks stats::filter()
✖ dplyr::lag()        masks stats::lag()
✖ purrr::when()       masks foreach::when()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(embed) # for target encoding
Loading required package: recipes

Attaching package: ‘recipes’

The following object is masked from ‘package:stringr’:

    fixed

The following object is masked from ‘package:stats’:

    step

> library(vroom)

Attaching package: ‘vroom’

The following objects are masked from ‘package:readr’:

    as.col_spec, col_character, col_date, col_datetime, col_double,
    col_factor, col_guess, col_integer, col_logical, col_number,
    col_skip, col_time, cols, cols_condense, cols_only, date_names,
    date_names_lang, date_names_langs, default_locale, fwf_cols,
    fwf_empty, fwf_positions, fwf_widths, locale, output_column,
    problems, spec

> library(DataExplorer)
> library(patchwork)
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ rsample      1.2.0
✔ dials        1.2.0     ✔ tune         1.1.2
✔ infer        1.0.5     ✔ workflows    1.1.3
✔ modeldata    1.2.0     ✔ workflowsets 1.0.1
✔ parsnip      1.1.1     ✔ yardstick    1.2.0
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ purrr::accumulate() masks foreach::accumulate()
✖ scales::discard()   masks purrr::discard()
✖ dplyr::filter()     masks stats::filter()
✖ recipes::fixed()    masks stringr::fixed()
✖ dplyr::lag()        masks stats::lag()
✖ yardstick::spec()   masks vroom::spec(), readr::spec()
✖ recipes::step()     masks stats::step()
✖ purrr::when()       masks foreach::when()
• Learn how to get started at https://www.tidymodels.org/start/
> library(discrim)

Attaching package: ‘discrim’

The following object is masked from ‘package:dials’:

    smoothness

> library(naivebayes)
naivebayes 0.9.7 loaded
> library(kknn)
> 
> 
> # Load the data -----------------------------------------------------------
> # setwd('~/College/Stat348/AmazonEmployeeAccess')
> 
> # Load data
> amazon_train <- vroom('./train.csv')
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> amazon_train$ACTION <- as.factor(amazon_train$ACTION)
> 
> amazon_test <- vroom('./test.csv')
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> # Create 2 exploratory plots ----------------------------------------------
> 
> # dplyr::glimpse(amazon_train)
> # plot_bar(amazon_train) # bar charts of all discrete variables
> # plot_histogram(amazon_train) # histograms of all numerical variables
> # # I see there are some with more coutns than others
> # plot_missing(amazon_train) # missing values
> 
> # Clean my data ###
> 
> # 
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
> #   step_dummy(all_nominal_predictors()) # dummy variable encoding
> #   # step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> # # also step_lencode_glm() and step_lencode_bayes()
> # 
> # # # apply the recipe to your data
> # # prep <- prep(my_recipe)
> # # train_clean <- bake(prep, new_data = amazon_train)
> # # 
> # # # -------------------------------------------------------------------------
> # # 
> # # 
> # # #Do Mosaic plot
> # # #Do chi-square table
> # # 
> # # # 112 is the answer
> # # 
> # # 
> # # # LOGISTIC REGRESSION -----------------------------------------------------
> # # 
> # # # Create model
> # # my_mod <- logistic_reg() %>% #Type of model
> # #   set_engine("glm")
> # # 
> # # #Define Worflow
> # # amazon_workflow <- workflow() %>% 
> # #   add_recipe(my_recipe) %>%
> # #   add_model(my_mod) %>%
> # #   fit(data = amazon_train) # Fit the workflow
> # # 
> # # # Get Predictions
> # # amazon_predictions <- predict(amazon_workflow,
> # #                               new_data=amazon_test,
> # #                               type="prob") # "class" or "prob" (see doc)
> # # 
> # # # # Make histogram to determine cut off
> # # # hist(amazon_predictions$.pred_1)
> # # # 
> # # # # Get action column with cut off
> # # # action <- amazon_predictions %>%
> # # #   mutate(Action = ifelse(.pred_1 > 0.99, 1, 0))
> # # 
> # # # Format table
> # # 
> # # 
> # # 
> # # amazon_test$Action <- amazon_predictions$.pred_1
> # # results <- amazon_test %>%
> # #   rename(Id = id) %>%
> # #   select(Id, Action)
> # # 
> # # 
> # # # get csv file
> # # vroom_write(results, 'AmazonPredsreg.csv', delim = ",")  
> # #   
> # # 
> # # # PENALIZED LOGISTIC REGRESSION -------------------------------------------
> # # # ROC across all posible cut offs
> # # 
> # # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> # #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> # #   step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> # #   # step_dummy(all_nominal_predictors()) # dummy variable encoding
> # #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> # # 
> # # my_mod <- logistic_reg(mixture=tune(), penalty=tune()) %>% #Type of model
> # #   set_engine("glmnet")
> # # 
> # # amazon_workflow <- workflow() %>%
> # # add_recipe(my_recipe) %>%
> # # add_model(my_mod)
> # # 
> # # ## Grid of values to tune over
> # # tuning_grid <- grid_regular(penalty(),
> # #                             mixture(),
> # #                             levels = 5) ## L^2 total tuning possibilities
> # # 
> # # ## Split data for CV
> # # folds <- vfold_cv(amazon_train, v = 3, repeats=1)
> # # 
> # # ## Run the CV
> # # CV_results <- amazon_workflow %>%
> # # tune_grid(resamples=folds,
> # #           grid=tuning_grid,
> # #           metrics=metric_set(roc_auc)) # they will all use a cut off of .5
> # # 
> # # ## Find best tuning parameters
> # # bestTune <- CV_results %>% 
> # #   select_best("roc_auc")
> # # 
> # # ## Finalize workflow and predict
> # # final_wf <- amazon_workflow %>% 
> # #   finalize_workflow(bestTune) %>% 
> # #   fit(data=amazon_train)
> # # 
> # # amazon_predictions <- final_wf %>%
> # #   predict(new_data = amazon_test, type = "prob")
> # # 
> # # # Get action column with cut off
> # # # action <- amazon_predictions %>%
> # # #   mutate(Action = ifelse(.pred_class > 0.99, 1, 0))
> # # 
> # # # Format table
> # # amazon_test$Action <- amazon_predictions$.pred_1
> # # results <- amazon_test %>%
> # #   rename(Id = id) %>%
> # #   select(Id, Action)
> # # 
> # # 
> # # # get csv file
> # # vroom_write(results, 'AmazonPredspreg.csv', delim = ",")  
> # # # penalty is first and mixture is second
> # # 
> # # 
> # # 
> # 
> # 
> # # RANDOM FOREST  ----------------------------------------------------------
> # 
> # 
> # my_mod <- rand_forest(mtry = tune(),
> #                       min_n=tune(),
> #                       trees=250) %>%
> #   set_engine("ranger") %>%
> #   set_mode("classification")
> # 
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor)  %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
> #   step_dummy(all_nominal_predictors()) # dummy variable encoding
> # # step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> # 
> # ## Create a workflow with model & recipe
> # 
> # amazon_workflow <- workflow() %>% 
> #   add_recipe(my_recipe) %>%
> #   add_model(my_mod) %>%
> #   fit(data = amazon_train)
> # 
> # ## Set up grid of tuning values
> # 
> # tuning_grid <- grid_regular(mtry(range = c(1,ncol(amazon_train)-1)), # How many Variables to choose from 
> #                             # researches have found log of total variables is enough
> #                             min_n(), # Number of observations in a leaf
> #                             levels = 5)
> # 
> # ## Set up K-fold CV
> # folds <- vfold_cv(amazon_train, v = 10, repeats=1)
> # 
> # CV_results <- amazon_workflow %>%
> #   tune_grid(resamples=folds,
> #             grid=tuning_grid,
> #             metrics=metric_set(roc_auc))
> # 
> # ## Find best tuning parameters
> # bestTune <- CV_results %>% 
> #   select_best("roc_auc")
> # 
> # ## Finalize workflow and predict
> # 
> # final_wf <- amazon_workflow %>% 
> #   finalize_workflow(bestTune) %>% 
> #   fit(data=amazon_train)
> # 
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredspreg.csv', delim = ",")
> # 
> # 
> # # Naive Bayes -------------------------------------------------------------
> # 
> # 
> # ## nb model3
> # nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
> #   set_mode("classification") %>%
> #   set_engine("naivebayes") # install discrim library for the naivebayes eng
> # 
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor)  %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
> #   step_dummy(all_nominal_predictors())
> # 
> # nb_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(nb_model)
> # 
> # ## Tune smoothness and Laplace here
> # tuning_grid <- grid_regular(smoothness(),
> #                             Laplace(),
> #                             levels = 5)
> # 
> # ## Set up K-fold CV
> # folds <- vfold_cv(amazon_train, v = 10, repeats=1)
> # 
> # CV_results <- nb_wf %>%
> #   tune_grid(resamples=folds,
> #             grid=tuning_grid,
> #             metrics=metric_set(MSE))
> # 
> # ## Find best tuning parameters
> # bestTune <- CV_results %>% 
> #   select_best("roc_auc")
> # 
> # ## Finalize workflow and predict
> # 
> # final_wf <- nb_wf %>% 
> #   finalize_workflow(bestTune) %>% 
> #   fit(data=amazon_train)
> # 
> # ## Predict
> # #predict(nb_wf, new_data=myNewData, type=)
> # 
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # #save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredspreg.csv', delim = ",")
> 
> 
> # KNN ---------------------------------------------------------------------
> 
> 
> ## knn model
> knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kknn")
> 
> ## Reciepe
> my_recipe <- recipe(ACTION~., data=amazon_train) %>%
+   step_mutate_at(all_numeric_predictors(), fn = factor)  %>% # turn all numeric features into factors
+   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
+   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>%
+   step_normalize(all_numeric_predictors())
> 
> ## Workflow
> knn_wf <- workflow() %>%
+   add_recipe(my_recipe) %>%
+   add_model(knn_model)
> 
> ## Tune
> tuning_grid <- grid_regular(neighbors(),
+                             levels = 10)
> 
> ## Set up K-fold CV
> folds <- vfold_cv(amazon_train, v = 10, repeats=1)
> 
> CV_results <- knn_wf %>%
+   tune_grid(resamples=folds,
+             grid=tuning_grid,
+             metrics=metric_set(roc_auc))
> 
> ## Find best tuning parameters
> bestTune <- CV_results %>% 
+   select_best("roc_auc")
> 
> ## Finalize workflow and predict
> 
> final_wf <- knn_wf %>% 
+   finalize_workflow(bestTune) %>% 
+   fit(data=amazon_train)
boundary (singular) fit: see help('isSingular')
Warning message:
Column(s) have zero variance so scaling cannot be used: `MGR_ID`. Consider using `step_zv()` to remove those columns before normalizing 
> 
> ## Fit or Tune Model HERE
> amazon_predictions <- final_wf %>%
+   predict(new_data = amazon_test, type = "prob")
> 
> save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> 
> # Format table
> amazon_test$Action <- amazon_predictions$.pred_1
> results <- amazon_test %>%
+   rename(Id = id) %>%
+   select(Id, Action)
> 
> vroom_write(results, 'AmazonPredspreg.csv', delim = ",")
> stopCluster(cl)
> 
> proc.time()
   user  system elapsed 
 89.970   1.471 268.641 
