
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> num_cores <- parallel::detectCores() #How many cores do I have?
> cl <- makePSOCKcluster(num_cores)
> registerDoParallel(cl)
> 
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.0
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ purrr::accumulate() masks foreach::accumulate()
✖ dplyr::filter()     masks stats::filter()
✖ dplyr::lag()        masks stats::lag()
✖ purrr::when()       masks foreach::when()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(embed) # for target encoding
Loading required package: recipes

Attaching package: ‘recipes’

The following object is masked from ‘package:stringr’:

    fixed

The following object is masked from ‘package:stats’:

    step

> library(vroom)

Attaching package: ‘vroom’

The following objects are masked from ‘package:readr’:

    as.col_spec, col_character, col_date, col_datetime, col_double,
    col_factor, col_guess, col_integer, col_logical, col_number,
    col_skip, col_time, cols, cols_condense, cols_only, date_names,
    date_names_lang, date_names_langs, default_locale, fwf_cols,
    fwf_empty, fwf_positions, fwf_widths, locale, output_column,
    problems, spec

> library(DataExplorer)
> library(patchwork)
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ rsample      1.2.0
✔ dials        1.2.0     ✔ tune         1.1.2
✔ infer        1.0.5     ✔ workflows    1.1.3
✔ modeldata    1.2.0     ✔ workflowsets 1.0.1
✔ parsnip      1.1.1     ✔ yardstick    1.2.0
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ purrr::accumulate() masks foreach::accumulate()
✖ scales::discard()   masks purrr::discard()
✖ dplyr::filter()     masks stats::filter()
✖ recipes::fixed()    masks stringr::fixed()
✖ dplyr::lag()        masks stats::lag()
✖ yardstick::spec()   masks vroom::spec(), readr::spec()
✖ recipes::step()     masks stats::step()
✖ purrr::when()       masks foreach::when()
• Search for functions across packages at https://www.tidymodels.org/find/
> library(discrim)

Attaching package: ‘discrim’

The following object is masked from ‘package:dials’:

    smoothness

> library(naivebayes)
naivebayes 0.9.7 loaded
> library(kknn)
> library(themis)
> 
> 
> # Load the data -----------------------------------------------------------
> # setwd('~/College/Stat348/AmazonEmployeeAccess')
> 
> # Load data
> amazon_train <- vroom('./train.csv')
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> amazon_train$ACTION <- as.factor(amazon_train$ACTION)
> 
> amazon_test <- vroom('./test.csv')
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> # Create 2 exploratory plots ----------------------------------------------
> 
> # dplyr::glimpse(amazon_train)
> # plot_bar(amazon_train) # bar charts of all discrete variables
> # plot_histogram(amazon_train) # histograms of all numerical variables
> # # I see there are some with more coutns than others
> # plot_missing(amazon_train) # missing values
> 
> # Clean my data ####
> 
> # Set my receipe (it will be different for each model)
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
> #   step_dummy(all_nominal_predictors()) # dummy variable encoding
> #   # step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> # # also step_lencode_glm() and step_lencode_bayes()
> 
> # # apply the recipe to your data
> # prep <- prep(my_recipe)
> # train_clean <- bake(prep, new_data = amazon_train)
> 
> #
> # # LOGISTIC REGRESSION -----------------------------------------------------
> # 
> # # Set my recipe
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
> #   step_normalize(all_numeric_predictors()) %>%
> #   step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
> #   step_smote(all_outcomes(), neighbors=5)
> # 
> # # Create model
> # lg_mod <- logistic_reg() %>% #Type of model
> #   set_engine("glm")
> # 
> # #Define Worflow
> # amazon_workflow <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(lg_mod) %>%
> #   fit(data = amazon_train) # Fit the workflow
> # 
> # # Get Predictions
> # amazon_predictions <- predict(amazon_workflow,
> #                               new_data=amazon_test,
> #                               type="prob") # "class" or "prob" (see doc)
> # 
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredlg.csv', delim = ",")
> # 
> # 
> # # PENALIZED LOGISTIC REGRESSION-------------------------------------------
> # # ROC across all possible cut offs
> # 
> # # Recipe
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%''
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
> #   step_normalize(all_numeric_predictors()) %>%
> #   step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
> #   step_smote(all_outcomes(), neighbors=5)
> # 
> # # Model
> # plg_mod <- logistic_reg(mixture=tune(), penalty=tune()) %>% #Type of model
> #   set_engine("glmnet")
> # 
> # # Workflow
> # amazon_workflow <- workflow() %>% 
> #   add_recipe(my_recipe) %>% 
> #   add_model(plg_mod)
> # 
> # ## Grid of values to tune over
> # tuning_grid <- grid_regular(penalty(),
> #                             mixture(),
> #                             levels = 5) ## L^2 total tuning possibilities
> # 
> # ## Split data for CV
> # folds <- vfold_cv(amazon_train, v = 5, repeats=1)
> # 
> # ## Run the CV
> # CV_results <- amazon_workflow %>%
> #   tune_grid(resamples=folds,
> #           grid=tuning_grid,
> #           metrics=metric_set(roc_auc)) # they will all use a cut off of .5
> # 
> # ## Find best tuning parameters
> # bestTune <- CV_results %>%
> #   select_best("roc_auc")
> # 
> # ## Finalize workflow and predict
> # final_wf <- amazon_workflow %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data=amazon_train)
> # 
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredsplg.csv', delim = ",")
> # # penalty is first and mixture is second
> # 
> 
> # # RANDOM FOREST  ----------------------------------------------------------
> # 
> # # Recipe
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   # step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
> #   # step_normalize(all_numeric_predictors()) %>%
> #   # step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
> #   step_smote(all_outcomes(), neighbors=5)
> # 
> # # Model
> # rf_mod <- rand_forest(mtry = tune(),
> #                       min_n=tune(),
> #                       trees=250) %>%
> #   set_engine("ranger") %>%
> #   set_mode("classification")
> # 
> # ## Workflow
> # amazon_workflow <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(rf_mod) %>%
> #   fit(data = amazon_train)
> # 
> # ## Set up grid of tuning values
> # tuning_grid <- grid_regular(mtry(range = c(1,9)), # How many Variables to choose from
> #                             # researches have found log of total variables is enough
> #                             min_n(), 
> #                             levels = 5)
> # 
> # # Set up K-fold CV
> # folds <- vfold_cv(amazon_train, v = 5, repeats=1)
> # 
> # # Cross Validation
> # CV_results <- amazon_workflow %>%
> #   tune_grid(resamples=folds,
> #             grid=tuning_grid,
> #             metrics=metric_set(roc_auc))
> # 
> # # Find best tuning parameters
> # bestTune <- CV_results %>%
> #   select_best("roc_auc")
> # 
> # # Finalize workflow
> # final_wf <- amazon_workflow %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data=amazon_train)
> # 
> # # Predict
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # # save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredsrf.csv', delim = ",")
> 
> 
> # # Naive Bayes -------------------------------------------------------------
> # 
> # ## nb model3
> # nb_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>%
> #   set_mode("classification") %>%
> #   set_engine("naivebayes") # install discrim library for the naivebayes eng
> # 
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   #step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
> #   step_normalize(all_numeric_predictors()) %>%
> #   step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
> #   step_smote(all_outcomes(), neighbors=5)
> # 
> # # my_recipe_prep <- prep(my_recipe, data = amazon_train)
> # # baked_data <- bake(my_recipe, new_data = NULL)
> # 
> # nb_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(nb_model)
> # 
> # ## Tune smoothness and Laplace here
> # tuning_grid <- grid_regular(smoothness(),
> #                             Laplace(),
> #                             levels = 5)
> # 
> # ## Set up K-fold CV
> # folds <- vfold_cv(amazon_train, v = 5, repeats=1)
> # 
> # CV_results <- nb_wf %>%
> #   tune_grid(resamples=folds,
> #             grid=tuning_grid,
> #             metrics=metric_set(roc_auc))
> # 
> # ## Find best tuning parameters
> # bestTune <- CV_results %>%
> #   select_best("roc_auc")
> # 
> # ## Finalize workflow
> # final_wf <- nb_wf %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data=amazon_train)
> # 
> # ## Predict
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # #save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredsnb.csv', delim = ",")
> 
> # # KNN ---------------------------------------------------------------------
> # 
> # # knn model
> # knn_model <- nearest_neighbor(neighbors=tune()) %>% # set or tune
> #   set_mode("classification") %>%
> #   set_engine("kknn")
> # 
> # ## Recipe
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   # step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
> #   step_normalize(all_numeric_predictors()) %>%
> #   step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
> #   step_smote(all_outcomes(), neighbors=5)
> # 
> # ## Workflow
> # knn_wf <- workflow() %>%
> #   add_recipe(my_recipe) %>%
> #   add_model(knn_model)
> # 
> # ## Tune
> # tuning_grid <- grid_regular(neighbors(),
> #                             levels = 10)
> # 
> # ## Set up K-fold CV
> # folds <- vfold_cv(amazon_train, v = 5, repeats=1)
> # 
> # CV_results <- knn_wf %>%
> #   tune_grid(resamples=folds,
> #             grid=tuning_grid,
> #             metrics=metric_set(roc_auc))
> # 
> # ## Find best tuning parameters
> # bestTune <- CV_results %>%
> #   select_best("roc_auc")
> # 
> # ## Finalize workflow and predict
> # 
> # final_wf <- knn_wf %>%
> #   finalize_workflow(bestTune) %>%
> #   fit(data=amazon_train)
> # 
> # ## Fit or Tune Model HERE
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # # save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # vroom_write(results, 'AmazonPredsknn.csv', delim = ",")
> 
> 
> # Support Vector Machine --------------------------------------------------
> # Each point is a vector representing the X's
> # Support vector => vectors that we are goint to use to create the boundaries
> # Usually just for Binary Classification (heavy computation - make reduction)
> 
> # Kernel = How you compute the boundary matters
>   # Computation fast way of computing this things
>     # Linear
>     # Polynomial
>     # Radial
> 
> # SVM models
> svmPoly <- svm_poly(degree=tune(), cost=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kernlab")
> 
> svmRadial <- svm_rbf(rbf_sigma=tune(), cost=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kernlab")
> 
> svmLinear <- svm_linear(cost=tune()) %>% # set or tune
+   set_mode("classification") %>%
+   set_engine("kernlab")
> 
> ## Fit or Tune Model HERE
> 
> my_recipe <- recipe(ACTION~., data=amazon_train) %>%
+   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
+   step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
+   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) %>% #target encoding
+   step_normalize(all_numeric_predictors()) %>%
+   step_pca(all_predictors(), threshold=.9) %>% # Reduce your matrix
+   step_smote(all_outcomes(), neighbors=5) #Threshold is between 0 and 1
> 
> svmP_wf <- workflow() %>%
+   add_recipe(my_recipe) %>%
+   add_model(svmPoly)
> 
> svmR_wf <- workflow() %>%
+   add_recipe(my_recipe) %>%
+   add_model(svmRadial)
> 
> svmL_wf <- workflow() %>%
+   add_recipe(my_recipe) %>%
+   add_model(svmLinear)
> 
> ## Tune
> tuning_grid <- grid_regular(degree(),
+                             cost(),
+                             levels = 5)
> 
> ## Set up K-fold CV
> folds <- vfold_cv(amazon_train, v = 5, repeats=1)
> 
> p_results <- svmP_wf %>%
+   tune_grid(resamples=folds,
+             grid=tuning_grid,
+             metrics=metric_set(roc_auc))
