
R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library(doParallel)
Loading required package: foreach
Loading required package: iterators
Loading required package: parallel
> 
> num_cores <- parallel::detectCores() #How many cores do I have?
> cl <- makePSOCKcluster(num_cores)
> registerDoParallel(cl)
> 
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.3     ✔ readr     2.1.4
✔ forcats   1.0.0     ✔ stringr   1.5.0
✔ ggplot2   3.4.3     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.0
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ purrr::accumulate() masks foreach::accumulate()
✖ dplyr::filter()     masks stats::filter()
✖ dplyr::lag()        masks stats::lag()
✖ purrr::when()       masks foreach::when()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(embed) # for target encoding
Loading required package: recipes

Attaching package: ‘recipes’

The following object is masked from ‘package:stringr’:

    fixed

The following object is masked from ‘package:stats’:

    step

> library(vroom)

Attaching package: ‘vroom’

The following objects are masked from ‘package:readr’:

    as.col_spec, col_character, col_date, col_datetime, col_double,
    col_factor, col_guess, col_integer, col_logical, col_number,
    col_skip, col_time, cols, cols_condense, cols_only, date_names,
    date_names_lang, date_names_langs, default_locale, fwf_cols,
    fwf_empty, fwf_positions, fwf_widths, locale, output_column,
    problems, spec

> library(DataExplorer)
> library(patchwork)
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ rsample      1.2.0
✔ dials        1.2.0     ✔ tune         1.1.2
✔ infer        1.0.5     ✔ workflows    1.1.3
✔ modeldata    1.2.0     ✔ workflowsets 1.0.1
✔ parsnip      1.1.1     ✔ yardstick    1.2.0
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ purrr::accumulate() masks foreach::accumulate()
✖ scales::discard()   masks purrr::discard()
✖ dplyr::filter()     masks stats::filter()
✖ recipes::fixed()    masks stringr::fixed()
✖ dplyr::lag()        masks stats::lag()
✖ yardstick::spec()   masks vroom::spec(), readr::spec()
✖ recipes::step()     masks stats::step()
✖ purrr::when()       masks foreach::when()
• Use suppressPackageStartupMessages() to eliminate package startup messages
> 
> 
> # Load the data -----------------------------------------------------------
> # setwd('~/College/Stat348/AmazonEmployeeAccess')
> 
> # Load data
> amazon_train <- vroom('./train.csv')
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> amazon_train$ACTION <- as.factor(amazon_train$ACTION)
> 
> amazon_test <- vroom('./test.csv')
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> # Create 2 exploratory plots ----------------------------------------------
> 
> # dplyr::glimpse(amazon_train)
> # plot_bar(amazon_train) # bar charts of all discrete variables
> # plot_histogram(amazon_train) # histograms of all numerical variables
> # # I see there are some with more coutns than others
> # plot_missing(amazon_train) # missing values
> 
> # Clean my data ###
> 
> 
> my_recipe <- recipe(ACTION~., data=amazon_train) %>%
+   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
+   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
+   step_dummy(all_nominal_predictors()) # dummy variable encoding
>   # step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> # also step_lencode_glm() and step_lencode_bayes()
> 
> # # apply the recipe to your data
> # prep <- prep(my_recipe)
> # train_clean <- bake(prep, new_data = amazon_train)
> # 
> # # -------------------------------------------------------------------------
> # 
> # 
> # #Do Mosaic plot
> # #Do chi-square table
> # 
> # # 112 is the answer
> # 
> # 
> # # LOGISTIC REGRESSION -----------------------------------------------------
> # 
> # # Create model
> # my_mod <- logistic_reg() %>% #Type of model
> #   set_engine("glm")
> # 
> # #Define Worflow
> # amazon_workflow <- workflow() %>% 
> #   add_recipe(my_recipe) %>%
> #   add_model(my_mod) %>%
> #   fit(data = amazon_train) # Fit the workflow
> # 
> # # Get Predictions
> # amazon_predictions <- predict(amazon_workflow,
> #                               new_data=amazon_test,
> #                               type="prob") # "class" or "prob" (see doc)
> # 
> # # # Make histogram to determine cut off
> # # hist(amazon_predictions$.pred_1)
> # # 
> # # # Get action column with cut off
> # # action <- amazon_predictions %>%
> # #   mutate(Action = ifelse(.pred_1 > 0.99, 1, 0))
> # 
> # # Format table
> # 
> # 
> # 
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredsreg.csv', delim = ",")  
> #   
> # 
> # # PENALIZED LOGISTIC REGRESSION -------------------------------------------
> # # ROC across all posible cut offs
> # 
> # my_recipe <- recipe(ACTION~., data=amazon_train) %>%
> #   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
> #   step_other(all_nominal_predictors(), threshold = .001) %>% # combines categorical values that occur <5% into an "other" value
> #   # step_dummy(all_nominal_predictors()) # dummy variable encoding
> #   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> # 
> # my_mod <- logistic_reg(mixture=tune(), penalty=tune()) %>% #Type of model
> #   set_engine("glmnet")
> # 
> # amazon_workflow <- workflow() %>%
> # add_recipe(my_recipe) %>%
> # add_model(my_mod)
> # 
> # ## Grid of values to tune over
> # tuning_grid <- grid_regular(penalty(),
> #                             mixture(),
> #                             levels = 5) ## L^2 total tuning possibilities
> # 
> # ## Split data for CV
> # folds <- vfold_cv(amazon_train, v = 3, repeats=1)
> # 
> # ## Run the CV
> # CV_results <- amazon_workflow %>%
> # tune_grid(resamples=folds,
> #           grid=tuning_grid,
> #           metrics=metric_set(roc_auc)) # they will all use a cut off of .5
> # 
> # ## Find best tuning parameters
> # bestTune <- CV_results %>% 
> #   select_best("roc_auc")
> # 
> # ## Finalize workflow and predict
> # final_wf <- amazon_workflow %>% 
> #   finalize_workflow(bestTune) %>% 
> #   fit(data=amazon_train)
> # 
> # amazon_predictions <- final_wf %>%
> #   predict(new_data = amazon_test, type = "prob")
> # 
> # # Get action column with cut off
> # # action <- amazon_predictions %>%
> # #   mutate(Action = ifelse(.pred_class > 0.99, 1, 0))
> # 
> # # Format table
> # amazon_test$Action <- amazon_predictions$.pred_1
> # results <- amazon_test %>%
> #   rename(Id = id) %>%
> #   select(Id, Action)
> # 
> # 
> # # get csv file
> # vroom_write(results, 'AmazonPredspreg.csv', delim = ",")  
> # # penalty is first and mixture is second
> # 
> # 
> # 
> 
> 
> # RANDOM FOREST  ----------------------------------------------------------
> 
> 
> my_mod <- rand_forest(mtry = tune(),
+                       min_n=tune(),
+                       trees=250) %>%
+   set_engine("ranger") %>%
+   set_mode("classification")
> 
> my_recipe <- recipe(ACTION~., data=amazon_train) %>%
+   step_mutate_at(all_numeric_predictors(), fn = factor) %>% # turn all numeric features into factors
+   step_other(all_nominal_predictors(), threshold = .01) %>% # combines categorical values that occur <5% into an "other" value
+   step_dummy(all_nominal_predictors()) # dummy variable encoding
> # step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION)) #target encoding
> 
> ## Create a workflow with model & recipe
> 
> amazon_workflow <- workflow() %>% 
+   add_recipe(my_recipe) %>%
+   add_model(my_mod) %>%
+   fit(data = amazon_train)
Warning messages:
1: tune columns were requested but there were 111 predictors in the data. 111 will be used. 
2: tune samples were requested but there were 32769 rows in the data. 32769 will be used. 
> 
> ## Set up grid of tuning values
> 
> tuning_grid <- grid_regular(mtry(range = c(1,ncol(amazon_train)-1)), # How many Variables to choose from 
+                             # researches have found log of total variables is enough
+                             min_n(), # Number of observations in a leaf
+                             levels = 3)
> 
> ## Set up K-fold CV
> folds <- vfold_cv(amazon_train, v = 5, repeats=1)
> 
> CV_results <- amazon_workflow %>%
+   tune_grid(resamples=folds,
+             grid=tuning_grid,
+             metrics=metric_set(roc_auc))
> 
> ## Find best tuning parameters
> bestTune <- CV_results %>% 
+   select_best("roc_auc")
> 
> ## Finalize workflow and predict
> 
> final_wf <- amazon_workflow %>% 
+   finalize_workflow(bestTune) %>% 
+   fit(data=amazon_train)
> 
> amazon_predictions <- final_wf %>%
+   predict(new_data = amazon_test)
> 
> save(file="./MyFile.RData", list=c("amazon_predictions", "final_wf", "bestTune", "CV_results"))
> 
> colnames(amazon_predictions)
[1] ".pred_class"
> # Format table
> amazon_test$Action <- amazon_predictions$.pred_1
Warning message:
Unknown or uninitialised column: `.pred_1`. 
> results <- amazon_test %>%
+   rename(Id = id) %>%
+   select(Id, Action)
Error in `select()`:
! Can't subset columns that don't exist.
✖ Column `Action` doesn't exist.
Backtrace:
     ▆
  1. ├─amazon_test %>% rename(Id = id) %>% select(Id, Action)
  2. ├─dplyr::select(., Id, Action)
  3. ├─dplyr:::select.data.frame(., Id, Action)
  4. │ └─tidyselect::eval_select(expr(c(...)), data = .data, error_call = error_call)
  5. │   └─tidyselect:::eval_select_impl(...)
  6. │     ├─tidyselect:::with_subscript_errors(...)
  7. │     │ └─rlang::try_fetch(...)
  8. │     │   └─base::withCallingHandlers(...)
  9. │     └─tidyselect:::vars_select_eval(...)
 10. │       └─tidyselect:::walk_data_tree(expr, data_mask, context_mask)
 11. │         └─tidyselect:::eval_c(expr, data_mask, context_mask)
 12. │           └─tidyselect:::reduce_sels(node, data_mask, context_mask, init = init)
 13. │             └─tidyselect:::walk_data_tree(new, data_mask, context_mask)
 14. │               └─tidyselect:::as_indices_sel_impl(...)
 15. │                 └─tidyselect:::as_indices_impl(...)
 16. │                   └─tidyselect:::chr_as_locations(x, vars, call = call, arg = arg)
 17. │                     └─vctrs::vec_as_location(...)
 18. └─vctrs (local) `<fn>`()
 19.   └─vctrs:::stop_subscript_oob(...)
 20.     └─vctrs:::stop_subscript(...)
 21.       └─rlang::abort(...)
Execution halted
